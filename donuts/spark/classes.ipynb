{
 "metadata": {
  "name": "",
  "signature": "sha256:625004afee937c1d8fd52cf3b99702be172a8bb3ab658094ec95fef35d9c96ee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Module"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "import numpy.random as npr\n",
      "from StringIO import StringIO\n",
      "import math\n",
      "import numpy.testing as npt\n",
      "import numpy.random as npr\n",
      "from donuts.spark.fake import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Numpy conversion functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def np_to_str(a):\n",
      "    \"\"\"\n",
      "    Converts an np array to a binary string without newline\n",
      "    \"\"\"\n",
      "    a = np.array(a, dtype = np.float16)\n",
      "    si = StringIO()\n",
      "    np.save(si, a)\n",
      "    st =  si.getvalue()\n",
      "    return st\n",
      "\n",
      "def str_to_np(st): \n",
      "    \"\"\"\n",
      "    Converts a binary string to an np array\n",
      "    \"\"\"\n",
      "    return np.load(StringIO(st))\n",
      "\n",
      "def part_to_str(iterator):\n",
      "    arr = np.vstack(list(iterator))\n",
      "    st = np_to_str(arr)\n",
      "    return [st]\n",
      "\n",
      "def str_to_part(iterator):\n",
      "    sts = list(iterator)\n",
      "    return np.vstack([str_to_np(st) for st in sts])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Helper functions for data classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def partition_array3(arr, sz):\n",
      "    \"\"\"\n",
      "    Partitions a 4d array into a list of 4d arrays by the first 3 dimensions\n",
      "    \"\"\"\n",
      "    dims = np.shape(arr)\n",
      "    dim_parts = tuple([int(math.ceil(float(dims[i])/sz[i])) for i in range(3)])\n",
      "    nparts = np.prod(dim_parts)\n",
      "    temp = np.unravel_index(range(nparts), dim_parts)\n",
      "    lcorners = np.array([sz[i] * temp[i] for i in range(3)]).T\n",
      "    ucorners = np.array([sz[i] * temp[i] + sz[i] for i in range(3)]).T\n",
      "    for i in range(3):\n",
      "        ucorners[ucorners[:, i] > dims[i], i] = dims[i]\n",
      "    subarrs = [(tuple(lc), arr[lc[0]:uc[0], lc[1]:uc[1], lc[2]:uc[2]]) \\\n",
      "               for (lc, uc) in zip(lcorners, ucorners)]\n",
      "    return subarrs\n",
      "\n",
      "def _unpickled_to_subarr(tup):\n",
      "    return (tup[0], str_to_np(tup[1]))\n",
      "\n",
      "def _subarr_to_compressed(tup):\n",
      "    return (tup[0], np_to_str(tup[1]))\n",
      "\n",
      "def _apply_func(tup, f, inds):\n",
      "    if inds is None:\n",
      "        return (tup[0], np.apply_over_axes(f, tup[1], [3]))\n",
      "    else:\n",
      "        return (tup[0], np.apply_over_axes(f, tup[1][:, :, :, inds], [3]))\n",
      "    \n",
      "def _apply_funcs(tup, fs, inds):\n",
      "    if inds is None:\n",
      "        arrs = [np.apply_over_axes(f, tup[1], [3]) for f in fs]\n",
      "        arr = np.concatenate(arrs, 3)\n",
      "    else:\n",
      "        arrs = [np.apply_over_axes(f, tup[1][:, :, :, inds], [3]) for f in fs]\n",
      "        arr = np.concatenate(arrs, 3)\n",
      "    return (tup[0], arr)\n",
      "\n",
      "def _u_filter_c(tup, inds):\n",
      "    return (tup[0], np_to_str(str_to_np(tup[1])[:, :, :, inds]))\n",
      "\n",
      "def _aug_key(tup, ind):\n",
      "    return (tup[0], [ind, tup[1]])\n",
      "\n",
      "def _sort_combine(tup):\n",
      "    ll = len(tup[1])/2\n",
      "    itups = [tup[1][2*i] for i in range(ll)]\n",
      "    os = sorted(range(ll), key=itups.__getitem__)\n",
      "    sorted_els = [tup[1][2*i+1] for i in os]\n",
      "    arrs = [str_to_np(el) for el in sorted_els]\n",
      "    arr = np.concatenate(arrs, 3)\n",
      "    return (tup[0], arr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 203
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Specialized RDD container classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class VoxelPartition:\n",
      "    \n",
      "    def __init__(self, rdd = None, cont = None, picklef = None, arr = None, sz = None, \\\n",
      "                 parts = 10, picklefs = None, inds = None):\n",
      "        # 1. Set up needded parameters\n",
      "        if cont is None:\n",
      "            cont = FakeSparkContext()\n",
      "        self.cont = cont\n",
      "        if sz is None:\n",
      "            sz = (10, 10, 10)\n",
      "        self.sz = sz\n",
      "        if parts is None:\n",
      "            parts = 10\n",
      "        self.parts = parts\n",
      "        # 2. form RDD\n",
      "        if rdd is not None:\n",
      "            self.rdd = rdd\n",
      "        elif arr is not None:\n",
      "            self.rdd = cont.parallelize(partition_array3(arr, sz), parts)\n",
      "        elif picklefs is not None:\n",
      "            if inds is None:\n",
      "                rdds = [cont.pickleFile(picklefs[ind], parts).map(lambda x : _aug_key(x, ind))\\\n",
      "                        for ind in range(len(picklefs))]\n",
      "            else:\n",
      "                rdds = [cont.pickleFile(picklefs[ind], parts).map(lambda x : _u_filter_c(x, inds)).\\\n",
      "                        map(lambda x : _aug_key(x, ind))\\\n",
      "                        for ind in range(len(picklefs))]\n",
      "            new_rdd = cont.union(rdds).\\\n",
      "                combineByKey(lambda x : x, lambda x, y : x + y, lambda x, y: x + y)\n",
      "            self.rdd = new_rdd.map(_sort_combine)                \n",
      "        elif picklef is not None:\n",
      "            self.rdd = cont.pickleFile(picklef, parts).map(_unpickled_to_subarr)\n",
      "    \n",
      "    # computes a function on each voxel and returns an RDD with that result\n",
      "    def compute_quantity(self, func, inds = None):\n",
      "        new_rdd = self.rdd.map(lambda x : _apply_func(x, func, inds))\n",
      "        return VoxelPartition(cont = self.cont, sz = self.sz, parts = self.parts, rdd = new_rdd)\n",
      "    \n",
      "    def compute_quantities(self, lfuncs, inds = None):\n",
      "        new_rdd = self.rdd.map(lambda x : _apply_funcs(x, lfuncs, inds))\n",
      "        return VoxelPartition(cont = self.cont, sz = self.sz, parts = self.parts, rdd = new_rdd)\n",
      "    \n",
      "    def save_as_pickle_file(self, fname):\n",
      "        self.rdd.map(_subarr_to_compressed).saveAsPickleFile(fname)\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy as np\n",
      "    import numpy.random as npr\n",
      "    from donuts.spark.classes import *\n",
      "    dims = (25, 30, 20, 100)\n",
      "    arr1 = npr.normal(0, 1, dims)\n",
      "    arr2 = npr.normal(10, 1, dims)\n",
      "    sz = (10, 10, 10)\n",
      "    VoxelPartition(arr=arr1, sz=sz, cont = sc).save_as_pickle_file('arr1.pickle')\n",
      "    VoxelPartition(arr=arr2, sz=sz, cont = sc).save_as_pickle_file('arr2.pickle')\n",
      "    vp = VoxelPartition(cont = sc, picklefs = ['arr1.pickle', 'arr2.pickle'], inds = range(1,3))\n",
      "#    means = voxPart.compute_quantities([np.mean, np.var], range(1,3))\n",
      "#    means = voxPart.compute_quantity(np.mean, range(1,3))\n",
      "#    means.save_as_pickle_file('means.pickle')\n",
      "#    means2 = VoxelPartition(cont = sc, picklef = 'means.pickle')\n",
      "    np.shape(vp.rdd.first()[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o33.saveAsObjectFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 31, ip-172-31-12-114.us-west-2.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 198, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/root/spark/python/pyspark/serializers.py\", line 124, in dump_stream\n    for obj in iterator:\n  File \"/root/spark/python/pyspark/serializers.py\", line 187, in _batched\n    for item in iterator:\n  File \"/usr/lib64/python2.7/site-packages/donuts/spark/classes.py\", line 71, in _subarr_to_compressed\n    return (tup[0], np_to_str(tup[1]))\n  File \"/usr/lib64/python2.7/site-packages/donuts/spark/classes.py\", line 26, in np_to_str\n    a = np.array(a, dtype = float16)\nNameError: global name 'float16' is not defined\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-c83cf80c1be0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0marr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mVoxelPartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_as_pickle_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arr1.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mVoxelPartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_as_pickle_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arr2.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mvp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVoxelPartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpicklefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'arr1.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'arr2.pickle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/site-packages/donuts/spark/classes.pyc\u001b[0m in \u001b[0;36msave_as_pickle_file\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_as_pickle_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_subarr_to_compressed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsPickleFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/root/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsPickleFile\u001b[1;34m(self, path, batchSize)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsObjectFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.saveAsObjectFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 31, ip-172-31-12-114.us-west-2.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 107, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 98, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 198, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/root/spark/python/pyspark/serializers.py\", line 124, in dump_stream\n    for obj in iterator:\n  File \"/root/spark/python/pyspark/serializers.py\", line 187, in _batched\n    for item in iterator:\n  File \"/usr/lib64/python2.7/site-packages/donuts/spark/classes.py\", line 71, in _subarr_to_compressed\n    return (tup[0], np_to_str(tup[1]))\n  File \"/usr/lib64/python2.7/site-packages/donuts/spark/classes.py\", line 26, in np_to_str\n    a = np.array(a, dtype = float16)\nNameError: global name 'float16' is not defined\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}