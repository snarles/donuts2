{
 "metadata": {
  "name": "",
  "signature": "sha256:e95b9eb19e363cf1209bacfa1fb149672da874d3619ac14ff32f51cde028942b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Module"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def csv_row_to_array(st):\n",
      "    return np.array([float(s) for s in st.replace(',',' ').replace('  ',' ').split(' ')])\n",
      "\n",
      "def int_to_str(z):\n",
      "    \"\"\"\n",
      "    Converts a int into an ASCII string, using a base-90 representation\n",
      "    \n",
      "    Inputs:\n",
      "      z : int\n",
      "    Outputs:\n",
      "      str : the int represented as a string\n",
      "    \n",
      "    [Rule 1]: For z < 90, int2str(z) = chr(z + 33)\n",
      "    \n",
      "      int_to_str(0)  == chr(33) == '!'\n",
      "      int_to_str(1)  == chr(34) == '\"'\n",
      "      int_to_str(2)  == chr(35) == '#'\n",
      "      ...\n",
      "      int_to_str(89) == chr(122) == 'z'\n",
      "    \n",
      "    [Rule 2]: Represent larger nonnegative numbers by joining digits with chr(123) == '{'\n",
      "      90 == 1*90 + 0\n",
      "      int_to_str(90) == chr(34) + chr(123) + chr(33) == '\"{!'\n",
      "                        '1'      'join'     '0'\n",
      "    \n",
      "      91 == 1*90 + 1 \n",
      "      int_to_str(91) == chr(34) + chr(123) + chr(34) == '\"{\"'\n",
      "                        '1'      'join'     '1'\n",
      "    \n",
      "      92 == 1*90 + 2 \n",
      "      int_to_str(92) == chr(34) + chr(123) + chr(35) == '\"#\"'\n",
      "                        '1'      'join'     '2'\n",
      "    \n",
      "    And so on for higher powers:\n",
      "      16202 == 2*90**2 + 0*90 + 2\n",
      "      int_to_str(16202) == chr(35) + chr(123) + chr(33) + chr(123) + chr(35) == '#{!{#'\n",
      "                         '2'      'join'     '0'        'join'      '2'\n",
      "                         \n",
      "    [Rule 3]: Represent negative numbers by prepending chr(124) == '|'\n",
      "      int_to_str(91) == '\"{\"'\n",
      "      int_to_str(-91) == '|\"{\"'      \n",
      "      \n",
      "    [Note]:\n",
      "      chr(123) == '{' is reserved for joining digits\n",
      "      chr(124) == '|' is reserved for minus signs\n",
      "      chr(125) == '}' is reserved for designating the -log_10 floating point precision (default 0)\n",
      "      chr(126) == '~' is reserved for delimiting multiple CffStr\n",
      "      E.g. '!}\"{\"' == int2str(0) + '}' + int2str(91) denotes 91.0/(10**0) == 91.0\n",
      "           '#}\"{\"' == int2str(2) + '}' + int2str(91) denotes 91.0/(10**2) == 0.91\n",
      "    \"\"\"\n",
      "    z = int(z)\n",
      "    if (z < 0):\n",
      "        return chr(124) + int_to_str(-z)\n",
      "    if (z < 90):\n",
      "        return chr(z+33)\n",
      "    else:\n",
      "        resid = int(z % 90)\n",
      "        z = int(z-resid)/90\n",
      "        return int_to_str(z)+chr(90+33)+chr(resid+33)\n",
      "    \n",
      "def ints_to_str(zs):\n",
      "    \"\"\"\n",
      "    Converts an array or list of ints to a string.\n",
      "    \n",
      "    Inputs:\n",
      "      zs : list of ints\n",
      "    Outputs:\n",
      "      str : the list of ints represented as a string\n",
      "    \n",
      "    Example:\n",
      "      int_to_str(100) == '\"{+'\n",
      "      int_to_str(200) == '#{5'\n",
      "      ints_to_str([100, 200]) == '\"{+#{5'\n",
      "    See int2str(z)\n",
      "    \"\"\"\n",
      "    return ''.join(int_to_str(z) for z in zs)\n",
      "\n",
      "def floats_to_str(xs, precision):\n",
      "    \"\"\"\n",
      "    Converts an array or list of floats to a string.\n",
      "    \n",
      "    Inputs:\n",
      "      xs : list of floats\n",
      "      precision: the numerical precision will be equal to 1/(10**intRes)\n",
      "    Outputs:\n",
      "      str : the list of floats represented as a string\n",
      "    \"\"\"\n",
      "    zs = np.array(xs * float(10**precision), dtype=int)\n",
      "    return int_to_str(precision) + '}' + ints_to_str(zs)\n",
      "\n",
      "def multi_floats_to_str(xs_s, precision_s):\n",
      "    assert(len(xs_s)== len(precision_s))\n",
      "    return '~'.join([floats_to_str(xs_s[ii], precision_s[ii]) for ii in range(len(xs_s))])\n",
      "\n",
      "def str_to_floats(stt):\n",
      "    \"\"\"\n",
      "    Recovers the list of ints from the string representation.\n",
      "    See ints_to_str(zs)\n",
      "    \n",
      "    Inputs:\n",
      "      st: String composed of characters in [ord(33), ... ,ord(124)] == ['!',...,'~']\n",
      "    \"\"\"\n",
      "    st = stt.split('}')\n",
      "    if len(st)==1:\n",
      "        return np.array(str_to_ints(st[-1]), dtype=float)\n",
      "    if len(st)==2:\n",
      "        precision = str_to_ints(st[0])[0]\n",
      "        return np.array(str_to_ints(st[-1]), dtype=float)/float(10**precision)\n",
      "    \n",
      "def str_to_multi_floats(sts):\n",
      "    \"\"\"\n",
      "    Recovers a list of float arrays\n",
      "    \"\"\"\n",
      "    return [str_to_floats(st) for st in sts.split('~')]\n",
      "    \n",
      "def str_to_ints(st, maxlen = -1):\n",
      "    \"\"\"\n",
      "    Recovers the list of ints from the string representation.\n",
      "    See ints2str(zs)\n",
      "    \n",
      "    Inputs:\n",
      "      st: String composed of characters in [ord(33), ... ,ord(124)] == ['!',...,'|']\n",
      "    \"\"\"\n",
      "    os = [ord(c[0])-33 for c in st]\n",
      "    assert(min(os) >= 0 and max(os) <= 91) \n",
      "    zs = []\n",
      "    counter = 0\n",
      "    if maxlen == -1:\n",
      "        maxlen = len(os)\n",
      "    while (counter < len(os)) and (len(zs) < maxlen):\n",
      "        if os[counter] == 90: # 'join' symbol\n",
      "            zs[-1] = zs[-1] * 90 + math.copysign(1, zs[-1]) * os[counter + 1]\n",
      "            counter = counter + 1\n",
      "        elif os[counter] == 91: # 'minus' symbol\n",
      "            zs.append(-os[counter+1])    \n",
      "            counter = counter + 1\n",
      "        else:\n",
      "            zs.append(os[counter])\n",
      "        counter = counter + 1\n",
      "    return [int(z) for z in zs]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CffStr(str):\n",
      "    \"\"\"\n",
      "    A class representing a float array (or list thereof) represented as a compressed str.\n",
      "    See functions ints2str, str2ints\n",
      "    Intialization:\n",
      "      Option 1) Initialize using a string\n",
      "      Option 2) Initialize using an integer or int array\n",
      "      Option 3) Initialize using a dictionary (initOpts)\n",
      "                Dictionary values:\n",
      "                  value (optional): the compressed data string (of type str)\n",
      "                                    if NOT included, must include 'coords' and 'floats' OR 'coords' and 'ints'\n",
      "                  cffs (optional): a list of cffs or strings\n",
      "                  ints (optional):   an integer list or array\n",
      "                  floats (optional): floating-point values\n",
      "                                     if included, must also include intRes\n",
      "                  multi_floats (optional): an array or list of floats\n",
      "                  precision (optional): the numerical precision is parts per 10**intRes\n",
      "                  \n",
      "    \"\"\"\n",
      "    \n",
      "    def __new__(cls, initOpts):\n",
      "        value = 'emptyCFF'\n",
      "        if type(initOpts) == str:\n",
      "            value = initOpts\n",
      "        elif type(initOpts) == unicode:\n",
      "            value = str(initOpts)\n",
      "        elif type(initOpts) == int:\n",
      "            value = int_to_str(initOpts)\n",
      "        elif str(type(initOpts)) in [\"<type 'numpy.ndarray'>\", \"<type 'list'>\", \"<type 'tuple'>\"]:\n",
      "            value = ints_to_str(initOpts)\n",
      "        elif type(initOpts) == dict:\n",
      "            if initOpts.has_key('value'):\n",
      "                value = initOpts['value']\n",
      "            elif initOpts.has_key('cffs'):\n",
      "                value = '~'.join(initOpts['cffs'])\n",
      "            elif initOpts.has_key('ints'):\n",
      "                value = ints_to_str(initOpts['ints'])\n",
      "            elif initOpts.has_key('floats'):\n",
      "                assert(initOpts.has_key('precision'))\n",
      "                value = floats_to_str(initOpts['floats'], initOpts['precision'])\n",
      "            elif initOpts.has_key('multi_floats'):\n",
      "                assert(initOpts.has_key('precision'))\n",
      "                value = multi_floats_to_str(initOpts['multi_floats'], initOpts['precision'])\n",
      "        elif 'CffStr' in str(type(initOpts)):\n",
      "            value = str(initOpts)\n",
      "        return str.__new__(cls, value)\n",
      "    \n",
      "    def get_value(self):\n",
      "        return str(self)\n",
      "    \n",
      "    def get_ints(self):\n",
      "        assert('~' not in str(self))\n",
      "        st = str(self).split('}')[-1]\n",
      "        return str_to_ints(st)\n",
      "    \n",
      "    def get_floats(self):\n",
      "        return np.hstack(str_to_multi_floats(str(self)))\n",
      "    \n",
      "    def get_multi_floats(self):\n",
      "        return str_to_multi_floats(str(self))\n",
      "    \n",
      "    def get_array(self):\n",
      "        return np.array(str_to_multi_floats(str(self)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    c1 = CffStr({'floats': np.array([-5.1, 2.2, 1.3]), 'precision': 2})\n",
      "    c2 = CffStr({'ints': [1, 2, 3]})\n",
      "    c3 = CffStr((-121, 34122, 140))\n",
      "    c4 = CffStr({'cffs': [c1, c2, c3]})\n",
      "    print(c1.get_floats())\n",
      "    print(c2.get_floats())\n",
      "    print(c3.get_ints())\n",
      "    print(c4.get_array())\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-5.09  2.2   1.3 ]\n",
        "[ 1.  2.  3.]\n",
        "[-121, 34122, 140]\n",
        "[[ -5.09e+00   2.20e+00   1.30e+00]\n",
        " [  1.00e+00   2.00e+00   3.00e+00]\n",
        " [ -1.21e+02   3.41e+04   1.40e+02]]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def create_regions(lower, upper, gap, width):\n",
      "    \"\"\"\n",
      "    Creates overlapping 3D cubic regions which cover a 'greater region'\n",
      "    \n",
      "    Inputs:\n",
      "      lower: (ints) lower corner of greater region\n",
      "      upper: (ints) upper corner of greater region\n",
      "      gap:   (ints) gap between centers of small regions\n",
      "      width: (ints) width of small region\n",
      "    \n",
      "    Output:\n",
      "      ans:   ? x 3 x 3 np array\n",
      "      ans[:, :, 0] is the X breakpoints\n",
      "      ans[:, :, 1] is the Y breakpoints, etc.\n",
      "      ans[:, 0, :] is the lower breaks\n",
      "      ans[:, 1, :] is the upper breaks\n",
      "      ans[:, 2, 0] is the centroid coordinate\n",
      "    \"\"\"\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Voxel(tuple):\n",
      "    \"\"\"\n",
      "    A class representing a single 3-dimensional voxel.\n",
      "    Attributes are coordinates (key) and data. Optionally: cached data.\n",
      "    Internally, it uses characters to compress integer-valued data.\n",
      "    Initialize with a compressed string of CFF format.\n",
      "    Intialization:\n",
      "      Option 1) Initialize using a compressed data string (of type str)\n",
      "      Option 2) Initialize using a (Key, Value) str tuple (as is done by Spark automatically)\n",
      "                Same as option 1, with Value used as string\n",
      "      Option 3) Initialize using a dictionary (initOpts)\n",
      "                Dictionary values:\n",
      "                  intRes: the numerical precision for floats\n",
      "                INCLUDE:\n",
      "                  csv_row: a row read from a CSV file\n",
      "                  ncoords: the number of coordinates in the CSV file, default 3\n",
      "                OR INCLUDE:\n",
      "                  coords: a tuple containing the coords\n",
      "                  floats: floating-point values\n",
      "    \"\"\"\n",
      "    def __new__(cls, initOpts):        \n",
      "        if type(initOpts) == unicode:\n",
      "            initOpts = str(initOpts)\n",
      "        if type(initOpts) == str:\n",
      "            temp = initOpts.split('~')\n",
      "            key = CffStr(temp[0])\n",
      "            value = CffStr('~'.join(temp[1:]))\n",
      "        elif type(initOpts) == tuple: # constructed via Spark deserialization\n",
      "            key = CffStr(initOpts[0])\n",
      "            value = CffStr(initOpts[1])\n",
      "        elif type(initOpts) == dict:\n",
      "            if initOpts.has_key('csv_row'):\n",
      "                ncoords = initOpts.get('ncoords', 3)\n",
      "                temp = csv_row_to_array(initOpts['csv_row'])\n",
      "                initOpts['coords'] = temp[:ncoords]\n",
      "                initOpts['floats'] = temp[ncoords:]\n",
      "            key = CffStr(initOpts['coords'])\n",
      "            value = CffStr(initOpts)\n",
      "        kv = tuple((key, value))\n",
      "        assert(len(kv)==2)\n",
      "        return super(Voxel, cls).__new__(cls, tuple(kv))\n",
      "    \n",
      "    # creates a new voxel from float data\n",
      "    def convertData(self, floats, precision, coords = None):\n",
      "        if coords is None:\n",
      "            coords = self.get_coords()\n",
      "        newVoxel = Voxel({'coords': self.get_coords(), 'floats': floats, 'precision': precision})\n",
      "        return newVoxel\n",
      "    \n",
      "    def get_coords(self):\n",
      "        coords = tuple(self[0].get_ints())\n",
      "        return coords\n",
      "    \n",
      "    def get_floats(self):\n",
      "        return self[1].get_floats()\n",
      "\n",
      "    def get_multi_floats(self):\n",
      "        return self[1].get_multi_floats()\n",
      "\n",
      "    def get_array(self):\n",
      "        return self[1].get_array()\n",
      "    \n",
      "    def bare_copy(self):\n",
      "        return (str(self[0]), str(self[1]))\n",
      "    \n",
      "    def to_string(self): # used for writing to flat files\n",
      "        return str(self[0])+'~'+str(self[1])\n",
      "    \n",
      "    def to_csv_string(self, delimiter=','): # used for writing to flat files\n",
      "        return delimiter.join([str(v) for v in np.hstack([self.get_coords(), self.get_floats()])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    m = Voxel({'coords': (1, 2, 3), 'floats': np.array([1.12, 3.3, -4.5]), 'precision': 2})\n",
      "    print(m.get_coords())\n",
      "    print(m.get_floats())\n",
      "    bc = m.bare_copy()\n",
      "    print(bc)\n",
      "    m_clone = Voxel(bc)\n",
      "    print(m, m_clone)\n",
      "    print(m.to_csv_string())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 2, 3)\n",
        "[ 1.12  3.3  -4.5 ]\n",
        "('\"#$', '#}\"{7${]|&{!')\n",
        "(('\"#$', '#}\"{7${]|&{!'), ('\"#$', '#}\"{7${]|&{!'))\n",
        "1.0,2.0,3.0,1.12,3.3,-4.5\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy.random as npr\n",
      "    from StringIO import StringIO\n",
      "    si = StringIO()\n",
      "    # define functions used in testing\n",
      "    nvox = 100\n",
      "    def gen_vox():\n",
      "        coords = npr.randint(0, 10, 3)\n",
      "        data = npr.randn(20)\n",
      "        return np.hstack([coords, data])\n",
      "    # simulate text file\n",
      "    rawvoxes= np.array([gen_vox() for ii in range(nvox)])\n",
      "    np.savetxt(si, rawvoxes)\n",
      "    rawdata = si.getvalue().strip().split('\\n')\n",
      "    voxes = [Voxel({'precision': 3, 'csv_row': rawdat}) for rawdat in rawdata]\n",
      "    np.set_printoptions(precision=2)\n",
      "    print(rawvoxes[50])\n",
      "    print(\"\")\n",
      "    print(voxes[50].to_csv_string())\n",
      "    print(\"\")\n",
      "    print(Voxel(voxes[50].bare_copy()).to_csv_string())\n",
      "    print(\"\")\n",
      "    print(Voxel(voxes[50].to_string()).to_csv_string())\n",
      "    print(\"\")\n",
      "    keys = [k for (k, v) in voxes]\n",
      "    print(keys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 4.    9.    6.   -0.02  3.05 -1.12 -0.07 -0.55 -0.54 -0.19  0.07  0.12\n",
        "  0.15  1.26 -1.57  0.59  0.9   0.13 -0.62 -0.11 -0.26 -0.85  1.91]\n",
        "\n",
        "4.0,9.0,6.0,-0.024,3.05,-1.118,-0.07,-0.545,-0.541,-0.189,0.07,0.116,0.151,1.262,-1.566,0.59,0.896,0.134,-0.616,-0.109,-0.262,-0.849,1.908\n",
        "\n",
        "4.0,9.0,6.0,-0.024,3.05,-1.118,-0.07,-0.545,-0.541,-0.189,0.07,0.116,0.151,1.262,-1.566,0.59,0.896,0.134,-0.616,-0.109,-0.262,-0.849,1.908\n",
        "\n",
        "4.0,9.0,6.0,-0.024,3.05,-1.118,-0.07,-0.545,-0.541,-0.189,0.07,0.116,0.151,1.262,-1.566,0.59,0.896,0.134,-0.616,-0.109,-0.262,-0.849,1.908\n",
        "\n",
        "['!$#', '*!$', \"'#&\", \"'**\", '((*', '(((', '$$)', '*#*', \"%')\", \")')\", '&(#', ')%\"', ')**', '\\'\"&', '&#)', '(\"#', ')$$', '!*)', '&)%', ')%$', '($\"', '()%', '(&$', '\"\"\"', '#)*', \"$'&\", '!*(', '%)!', '$)\"', \"$')\", ')%*', '#%*', \"'#*\", \"$'$\", '*\"(', \"'*(\", \")''\", '%*(', \"''!\", '*&$', '\\'*\"', '#**', ')#*', ')\"!', '#&$', '!$)', '!&\"', '$(!', \"'%!\", '\"!*', \"%*'\", '%$*', '!%#', '!#%', '*)%', \"')&\", '*$&', '(#&', ')*\"', '%#)', '!\"%', \"'#)\", '\"&&', ')($', '\"%)', '!#$', '&##', ')#!', '($\"', '&$!', '*#&', '%(&', \"'$#\", '**\"', '$()', '\"\\'&', '&##', '%$%', \"*'#\", '***', '($(', '*\"&', '(&$', '()(', '((\"', '\"%*', '#!(', '*\"!', '*$&', '&&#', '#*%', '#$)', '#*!', '(((', '$%*', '!!(', '(($', '##(', \"'#'\", '\"#$']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing Import"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy.random as npr\n",
      "    from donuts.spark.classes import Voxel\n",
      "    from StringIO import StringIO\n",
      "    si = StringIO()\n",
      "    # define functions used in testing\n",
      "    nvox = 100\n",
      "    def gen_vox():\n",
      "        coords = npr.randint(0, 10, 3)\n",
      "        data = npr.randn(20)\n",
      "        return np.hstack([coords, data])\n",
      "    # simulate text file\n",
      "    rawvoxes= np.array([gen_vox() for ii in range(nvox)])\n",
      "    np.savetxt(si, rawvoxes)\n",
      "    rawdata = si.getvalue().strip().split('\\n')\n",
      "    voxes = [Voxel({'precision': 3, 'csv_row': rawdat}) for rawdat in rawdata]\n",
      "    np.set_printoptions(precision=2)\n",
      "    print(rawvoxes[50])\n",
      "    print(\"\")\n",
      "    print(voxes[50].to_csv_string())\n",
      "    print(\"\")\n",
      "    print(Voxel(voxes[50].bare_copy()).to_csv_string())\n",
      "    print(\"\")\n",
      "    print(Voxel(voxes[50].to_string()).to_csv_string())\n",
      "    print(\"\")\n",
      "    keys = [k for (k, v) in voxes]\n",
      "    print(keys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 2.    9.    8.    0.55 -0.25 -1.37 -0.38  1.31  1.33  0.48 -1.26 -0.83\n",
        " -0.34  1.1  -0.08 -1.08  0.12  2.13  0.99 -0.89 -1.38  0.11 -0.43]\n",
        "\n",
        "2.0,9.0,8.0,0.554,-0.254,-1.366,-0.375,1.306,1.326,0.481,-1.255,-0.831,-0.335,1.104,-0.083,-1.081,0.124,2.128,0.987,-0.888,-1.381,0.11,-0.426\n",
        "\n",
        "2.0,9.0,8.0,0.554,-0.254,-1.366,-0.375,1.306,1.326,0.481,-1.255,-0.831,-0.335,1.104,-0.083,-1.081,0.124,2.128,0.987,-0.888,-1.381,0.11,-0.426\n",
        "\n",
        "2.0,9.0,8.0,0.554,-0.254,-1.366,-0.375,1.306,1.326,0.481,-1.255,-0.831,-0.335,1.104,-0.083,-1.081,0.124,2.128,0.987,-0.888,-1.381,0.11,-0.426\n",
        "\n",
        "['#&#', '%)!', '$$%', '#!$', \"'#*\", '!*!', '%*(', \"'(!\", '()#', '!%\"', \"$'%\", \"'#%\", '$!%', '!\"$', '*)(', \"!'(\", '!(&', '%\\'\"', '%#\"', '#)(', '(!%', '#$%', '&&!', '\"(#', ')\"$', '\\'\"!', '*\")', '(&*', '\"\"(', ')\"$', '))\"', '%\")', '*\\'\"', \"')!\", '()&', '\"*\"', '&\"*', \"#'$\", '!&$', '&&)', '!))', \"'&$\", \")%'\", \"''&\", '$*)', \"$''\", '($\"', '$!*', '\"$#', '\"\\'&', '#*)', '\\'&\"', '&%%', '%)%', '&!$', \"!'%\", '$\"#', '\"\"$', '&%*', \"&')\", '\"!)', '\"*#', '$\"#', '$)%', '!#(', \"')%\", '#*)', '))(', \"#!'\", '(&#', ')#$', '!#%', \"'(%\", '#&\"', '\"(\"', '#))', ')#$', '!\"!', '\\'\"#', ')!)', '*#&', \"'%)\", '()!', ')&%', '((#', '$&\"', '\"))', '#%(', ')&*', '\"!\"', '$\"#', '#!*', \"')(\", ')(%', '*((', \"$'!\", '\"$\\'', '!&!', '&)#', '(*\"']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing in Spark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\" and 'sc' in vars():\n",
      "    import donuts.spark.classes as dc\n",
      "    def f(x):\n",
      "        return Voxel({'precision': 3, 'csv_row': x})\n",
      "    raw_rdd = sc.parallelize(rawdata, 2)\n",
      "    voxes = raw_rdd.map(f).collect()\n",
      "    def v2c(v):\n",
      "        return str(v)\n",
      "    def c2c(c1, c2):\n",
      "        return c1 + '~~' + c2\n",
      "    combovoxes = raw_rdd.map(f).combineByKey(v2c, c2c, c2c).collect()\n",
      "    print(combovoxes[0][0])\n",
      "    print(combovoxes[0][1].split('~~'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o25.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ip-172-31-15-68.us-west-2.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 90, in main\n    command = pickleSer._read_with_length(infile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 151, in _read_with_length\n    return self.loads(obj)\n  File \"/root/spark/python/pyspark/serializers.py\", line 396, in loads\n    return cPickle.loads(obj)\nImportError: No module named donuts.spark.classes\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-cedeea26e7d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mVoxel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csv_row'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mraw_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mvoxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mv2c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/root/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m         \"\"\"\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m             \u001b[0mbytesInJava\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.collect.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ip-172-31-15-68.us-west-2.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 90, in main\n    command = pickleSer._read_with_length(infile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 151, in _read_with_length\n    return self.loads(obj)\n  File \"/root/spark/python/pyspark/serializers.py\", line 396, in loads\n    return cPickle.loads(obj)\nImportError: No module named donuts.spark.classes\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}