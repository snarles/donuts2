{
 "metadata": {
  "name": "",
  "signature": "sha256:a14d32373884689529c7d4c4d028145fa409fd63cd4790f75ee4c28bed18752b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Module"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "import numpy.random as npr\n",
      "from StringIO import StringIO\n",
      "import math\n",
      "import numpy.testing as npt\n",
      "import numpy.random as npr\n",
      "from donuts.spark.fake import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Numpy conversion functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def np_to_str(a):\n",
      "    \"\"\"\n",
      "    Converts an np array to a binary string without newline\n",
      "    \"\"\"\n",
      "    a = np.array(a, dtype = np.float16)\n",
      "    si = StringIO()\n",
      "    np.save(si, a)\n",
      "    st =  si.getvalue()\n",
      "    return st\n",
      "\n",
      "def str_to_np(st): \n",
      "    \"\"\"\n",
      "    Converts a binary string to an np array\n",
      "    \"\"\"\n",
      "    return np.load(StringIO(st))\n",
      "\n",
      "def part_to_str(iterator):\n",
      "    arr = np.vstack(list(iterator))\n",
      "    st = np_to_str(arr)\n",
      "    return [st]\n",
      "\n",
      "def str_to_part(iterator):\n",
      "    sts = list(iterator)\n",
      "    return np.vstack([str_to_np(st) for st in sts])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Helper functions for data classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def partition_array3(arr, sz):\n",
      "    \"\"\"\n",
      "    Partitions a 4d array into a list of 4d arrays by the first 3 dimensions\n",
      "    \"\"\"\n",
      "    dims = np.shape(arr)\n",
      "    dim_parts = tuple([int(math.ceil(float(dims[i])/sz[i])) for i in range(3)])\n",
      "    nparts = np.prod(dim_parts)\n",
      "    temp = np.unravel_index(range(nparts), dim_parts)\n",
      "    lcorners = np.array([sz[i] * temp[i] for i in range(3)]).T\n",
      "    ucorners = np.array([sz[i] * temp[i] + sz[i] for i in range(3)]).T\n",
      "    for i in range(3):\n",
      "        ucorners[ucorners[:, i] > dims[i], i] = dims[i]\n",
      "    subarrs = [(tuple(lc), arr[lc[0]:uc[0], lc[1]:uc[1], lc[2]:uc[2]]) \\\n",
      "               for (lc, uc) in zip(lcorners, ucorners)]\n",
      "    return subarrs\n",
      "\n",
      "def _unpickled_to_subarr(tup):\n",
      "    return (tup[0], str_to_np(tup[1]))\n",
      "\n",
      "def _subarr_to_compressed(tup):\n",
      "    return (tup[0], np_to_str(tup[1]))\n",
      "\n",
      "def _apply_func(tup, f, inds):\n",
      "    if inds is None:\n",
      "        return (tup[0], np.apply_over_axes(f, tup[1], [3]))\n",
      "    else:\n",
      "        return (tup[0], np.apply_over_axes(f, tup[1][:, :, :, inds], [3]))\n",
      "    \n",
      "def _apply_funcs(tup, fs, inds):\n",
      "    if inds is None:\n",
      "        arrs = [np.apply_over_axes(f, tup[1], [3]) for f in fs]\n",
      "        arr = np.concatenate(arrs, 3)\n",
      "    else:\n",
      "        arrs = [np.apply_over_axes(f, tup[1][:, :, :, inds], [3]) for f in fs]\n",
      "        arr = np.concatenate(arrs, 3)\n",
      "    return (tup[0], arr)\n",
      "\n",
      "def _u_filter_c(tup, inds):\n",
      "    return (tup[0], np_to_str(str_to_np(tup[1])[:, :, :, inds]))\n",
      "\n",
      "def _aug_key(tup, ind):\n",
      "    return (tup[0], [ind, tup[1]])\n",
      "\n",
      "def _sort_combine(tup):\n",
      "    ll = len(tup[1])/2\n",
      "    itups = [tup[1][2*i] for i in range(ll)]\n",
      "    os = sorted(range(ll), key=itups.__getitem__)\n",
      "    sorted_els = [tup[1][2*i+1] for i in os]\n",
      "    arrs = [str_to_np(el) for el in sorted_els]\n",
      "    arr = np.concatenate(arrs, 3)\n",
      "    return (tup[0], arr)\n",
      "\n",
      "def np_to_txt(tup):\n",
      "    \"\"\"\n",
      "    Converts an np array to unraveled txt string\n",
      "    \"\"\"\n",
      "    dims = np.shape(tup[1])\n",
      "    a = np.array(tup[1], dtype = np.float16).ravel()\n",
      "    si = StringIO()\n",
      "    np.savetxt(si, np.hstack([tup[0], dims, a]), fmt = '%.6e')\n",
      "    st =  si.getvalue()\n",
      "    st = st.replace('\\n',' ')\n",
      "    return st\n",
      "\n",
      "def txt_to_np(st): \n",
      "    \"\"\"\n",
      "    Converts a binary string to an np array\n",
      "    \"\"\"\n",
      "    st = st.replace(' ','\\n')\n",
      "    seq = np.loadtxt(StringIO(st))\n",
      "    dims = seq[3:7]\n",
      "    coords = seq[:3]\n",
      "    data = seq[7:]\n",
      "    ndata = np.prod(dims)\n",
      "    tup = (tuple(np.array(coords, dtype=int)), np.reshape(data[:ndata], tuple(dims)))\n",
      "    return tup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy as np\n",
      "    import numpy.random as npr\n",
      "    #from donuts.spark.classes import *\n",
      "    import os\n",
      "    import time\n",
      "    # function arguments\n",
      "    dims = (40, 40, 40, 20)\n",
      "    arr = npr.normal(0, 1, dims)\n",
      "    sz = (10, 10, 10)\n",
      "    tempf = 'temp1.txt'\n",
      "    # function 1\n",
      "    os.system('rm temp1.txt')\n",
      "    t1 = time.clock()\n",
      "    arrs = partition_array3(arr, sz)\n",
      "    strs = [np_to_txt(tup) for tup in arrs]\n",
      "    f = open(tempf, 'w')\n",
      "    f.write('\\n'.join(strs))\n",
      "    f.close()\n",
      "    time1 = time.clock() - t1\n",
      "    # function 2\n",
      "    tempf = 'temp2.txt'\n",
      "    os.system('rm temp2.txt')\n",
      "    t1 = time.clock()\n",
      "    arrs = partition_array3(arr, sz)\n",
      "    lenmax = np.prod(sz) * np.shape(arr)[3]\n",
      "    flatarr = np.array([np.hstack([tup[0], np.shape(tup[1]), tup[1].ravel(),\\\n",
      "        np.zeros(lenmax - np.prod(np.shape(tup[1])))]) for tup in arrs])\n",
      "    np.savetxt(tempf, flatarr)\n",
      "    time2 = time.clock() - t1\n",
      "    print((time1, time2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(8.130000000000003, 1.3299999999999983)\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    # function 1\n",
      "    f = open('temp1.txt', 'r')\n",
      "    st1 = f.read().split('\\n')\n",
      "    f.close()\n",
      "    f = open('temp2.txt', 'r')\n",
      "    st2 = f.read().split('\\n')\n",
      "    f.close()\n",
      "    print(str(txt_to_np(st1[0]))[0:100])\n",
      "    print(str(txt_to_np(st2[0]))[0:100])\n",
      "    print(str(arrs[0])[0:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((0, 0, 0), array([[[[  5.15625000e-01,  -5.41992200e-01,  -1.42285200e+00, ...,\n",
        "            4.34570\n",
        "((0, 0, 0), array([[[[  5.15676773e-01,  -5.42103556e-01,  -1.42326909e+00, ...,\n",
        "            4.34687"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "((0, 0, 0), array([[[[  5.15676773e-01,  -5.42103556e-01,  -1.42326909e+00, ...,\n",
        "            4.34687\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Specialized RDD container classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class VoxelPartition:\n",
      "    \n",
      "    def __init__(self, rdd = None, cont = None, picklef = None, sz = None, \\\n",
      "                 parts = 10, picklefs = None, inds = None, textf = None):\n",
      "        # 1. Set up needded parameters\n",
      "        if cont is None:\n",
      "            cont = FakeSparkContext()\n",
      "        self.cont = cont\n",
      "        if sz is None:\n",
      "            sz = (10, 10, 10)\n",
      "        self.sz = sz\n",
      "        if parts is None:\n",
      "            parts = 10\n",
      "        self.parts = parts\n",
      "        # 2. form RDD\n",
      "        if rdd is not None:\n",
      "            self.rdd = rdd\n",
      "        elif picklefs is not None:\n",
      "            rdds = []\n",
      "            for ind in range(len(picklefs)):\n",
      "                if inds is None:\n",
      "                    evalst = 'cont.pickleFile(picklefs[indx], parts).map(lambda x : _aug_key(x, indx))'\n",
      "                else:\n",
      "                    evalst = 'cont.pickleFile(picklefs[indx], parts).map(lambda x : _u_filter_c(x, inds)).\\\n",
      "                        map(lambda x : _aug_key(x, indx))'\n",
      "                rdd = eval(evalst.replace('indx', str(ind)))\n",
      "                rdds.append(rdd)\n",
      "            new_rdd = cont.union(rdds).\\\n",
      "                combineByKey(lambda x : x, lambda x, y : x + y, lambda x, y: x + y)\n",
      "            self.rdd = new_rdd.map(_sort_combine)                \n",
      "        elif picklef is not None:\n",
      "            self.rdd = cont.pickleFile(picklef, parts).map(_unpickled_to_subarr)\n",
      "        elif textf is not None:\n",
      "            rt = cont.textFile(textf, parts)\n",
      "            self.rdd = rt.map(txt_to_np)\n",
      "\n",
      "    # computes a function on each voxel and returns an RDD with that result\n",
      "    def compute_quantity(self, func, inds = None):\n",
      "        new_rdd = self.rdd.map(lambda x : _apply_func(x, func, inds))\n",
      "        return VoxelPartition(cont = self.cont, sz = self.sz, parts = self.parts, rdd = new_rdd)\n",
      "    \n",
      "    def compute_quantities(self, lfuncs, inds = None):\n",
      "        new_rdd = self.rdd.map(lambda x : _apply_funcs(x, lfuncs, inds))\n",
      "        return VoxelPartition(cont = self.cont, sz = self.sz, parts = self.parts, rdd = new_rdd)\n",
      "    \n",
      "    def save_as_pickle_file(self, fname):\n",
      "        self.rdd.map(_subarr_to_compressed).saveAsPickleFile(fname)\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Script for storing an array to txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "def convscript(arr, tempf = 'temp.txt', sz = (10, 10, 10), hadoop_dir = '/root/ephemeral-hdfs'):\n",
      "    \"\"\"\n",
      "    hadoop_dir: no slash on the end, don't include /bin\n",
      "    cont: spark context\n",
      "    \"\"\"\n",
      "    arrs = partition_array3(arr, sz)\n",
      "    arrs = partition_array3(arr, sz)\n",
      "    lenmax = np.prod(sz) * np.shape(arr)[3]\n",
      "    flatarr = np.array([np.hstack([tup[0], np.shape(tup[1]), tup[1].ravel(),\\\n",
      "        np.zeros(lenmax - np.prod(np.shape(tup[1])))]) for tup in arrs])\n",
      "    os.chdir(hadoop_dir + '/bin')\n",
      "    np.savetxt(tempf, flatarr)\n",
      "    print('Wrote to file...')\n",
      "    os.system('./hadoop fs -mkdir '+tempf+' temp/'+tempf)\n",
      "    os.system('./hadoop fs -rmr temp/'+tempf)\n",
      "    os.system('./hadoop fs -put '+tempf+' temp/'+tempf)\n",
      "    print('Copied to hadoop... temp/' + tempf)\n",
      "    os.system('rm '+tempf)\n",
      "    print('Cleaning up...')\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy as np\n",
      "    import numpy.random as npr\n",
      "    #from donuts.spark.classes import *\n",
      "    import os\n",
      "    import time\n",
      "    # function arguments\n",
      "    dims = (40, 40, 40, 20)\n",
      "    arr = npr.normal(0, 1, dims)\n",
      "    sz = (10, 10, 10)\n",
      "    convscript(arr, 'temp.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote to file...\n",
        "Copied to hadoop... temp/temp.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    rdd = sc.textFile('temp/temp.txt', 20).map(txt_to_np)\n",
      "    tup = rdd.takeSample(False, 1)[0]\n",
      "    coords = tup[0]\n",
      "    print(arr[coords[0], coords[1], coords[2], :])\n",
      "    print(tup[1][0, 0, 0, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-1.21480154  2.05813269  0.09221659  0.51116435  1.48773645  0.17734219\n",
        " -1.54529451  0.32421841 -2.19298452 -0.89569408  1.55204075  1.2392434\n",
        "  0.12399026  0.25706187  0.14875756  1.08297756  0.6335233  -0.83150431\n",
        "  1.08053339 -0.19214762]\n",
        "[-1.214844    2.058594    0.09222412  0.5112305   1.487305    0.1773682\n",
        " -1.544922    0.3242188  -2.193359   -0.8955078   1.551758    1.239258\n",
        "  0.1239624   0.2570801   0.1488037   1.083008    0.6333008  -0.831543\n",
        "  1.080078   -0.1921387 ]\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "rdds = []\n",
      "parts = 20\n",
      "picklefs = ['arr1.pickle', 'arr2.pickle']\n",
      "cont = sc\n",
      "ind = -1\n",
      "ind = ind + 1\n",
      "evalst = 'cont.pickleFile(picklefs[indx], parts).map(lambda x : _aug_key(x, indx))'\n",
      "rdd = eval(evalst.replace('indx', str(ind)))\n",
      "smp = rdd.takeSample(False, 1)\n",
      "rdds.append(rdd)\n",
      "new_rdd = cont.union(rdds).combineByKey(lambda x : x, lambda x, y : x + y, lambda x, y: x + y)\n",
      "smp = cont.union(rdds).takeSample(False, 10)\n",
      "smp[0][0]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy as np\n",
      "    import numpy.random as npr\n",
      "    from donuts.spark.classes import *\n",
      "    dims = (25, 30, 20, 100)\n",
      "    arr1 = npr.normal(0, 1, dims)\n",
      "    arr2 = npr.normal(10, 1, dims)\n",
      "    sz = (10, 10, 10)\n",
      "    VoxelPartition(arr=arr1, sz=sz, cont = sc).save_as_pickle_file('arr1.pickle')\n",
      "    VoxelPartition(arr=arr2, sz=sz, cont = sc).save_as_pickle_file('arr2.pickle')\n",
      "    vp = VoxelPartition(cont = sc, picklefs = ['arr1.pickle', 'arr2.pickle'], inds = range(1,3))\n",
      "#    means = voxPart.compute_quantities([np.mean, np.var], range(1,3))\n",
      "#    means = voxPart.compute_quantity(np.mean, range(1,3))\n",
      "#    means.save_as_pickle_file('means.pickle')\n",
      "#    means2 = VoxelPartition(cont = sc, picklef = 'means.pickle')\n",
      "    print(np.shape(vp.rdd.first()[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10, 10, 10, 4)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    import numpy as np\n",
      "    import numpy.random as npr\n",
      "    from donuts.spark.classes import *\n",
      "    import os\n",
      "    dims = (40, 40, 40, 20)\n",
      "    arr1 = npr.normal(0, 1, dims)\n",
      "    sz = (10, 10, 10)\n",
      "    convscript(arr1, 'temp1.txt')\n",
      "    vp = VoxelPartition(textf = 'temp/temp1.txt', cont=sc)\n",
      "    tup = vp.rdd.takeSample(False, 1)[0]\n",
      "    coords = tup[0]\n",
      "    print(arr1[coords[0], coords[1], coords[2], 0:100:10])\n",
      "    print(tup[1][0, 0, 0, 0:100:10])\n",
      "    os.chdir('/root/ephemeral-hdfs/bin')\n",
      "    os.system('./hadoop fs -rmr arr1.pickle')\n",
      "    vp.save_as_pickle_file('arr1.pickle')\n",
      "    arr2 = npr.normal(0, 1, dims)\n",
      "    convscript(arr2, 'temp2.txt')\n",
      "    vp = VoxelPartition(textf = 'temp/temp2.txt', cont=sc)\n",
      "    tup = vp.rdd.takeSample(False, 1)[0]\n",
      "    coords = tup[0]\n",
      "    print(arr2[coords[0], coords[1], coords[2], 0:100:10])\n",
      "    print(tup[1][0, 0, 0, 0:100:10])\n",
      "    os.chdir('/root/ephemeral-hdfs/bin')\n",
      "    os.system('./hadoop fs -rmr arr2.pickle')\n",
      "    vp.save_as_pickle_file('arr2.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote to file...\n",
        "Copied to hadoop... temp/temp1.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up...\n",
        "[-1.5292326  -0.05051426]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[-1.529297   -0.05050659]\n",
        "Wrote to file..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Copied to hadoop... temp/temp2.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up...\n",
        "[ 1.23431053 -0.20431332]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 1.234375  -0.2043457]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    main_arr = np.concatenate([arr1, arr2])\n",
      "    vpm = VoxelPartition(picklefs = ['arr1.pickle', 'arr2.pickle'], cont = sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}