{
 "metadata": {
  "name": "",
  "signature": "sha256:7f9dd282d1d659e04ff19a8824a513729860966cd0b5fd534cdfa4c097ee8c9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "import subprocess\n",
      "import donuts.spark.classes as dc\n",
      "import nibabel as nib\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir('/root/ephemeral-hdfs/bin')\n",
      "sz = (20, 20, 20)\n",
      "parts = 30"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s3names = ['s3://rawpredator/chris1/8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['s3://rawpredator/chris2/8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "innames = ['8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "tempnames = ['temp1_coil' + str(i) + '.txt' for i in range(0, 33)] + \\\n",
      "            ['temp2_coil' + str(i) + '.txt' for i in range(0, 33)]\n",
      "outnames = ['chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "           ['chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]\n",
      "s3outnames = ['s3://chris1data/chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "             ['s3://chris2data/chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = 0\n",
      "s3name = s3names[ind]\n",
      "inname = innames[ind]\n",
      "tempname = tempnames[ind]\n",
      "outname = outnames[ind]\n",
      "s3outname = s3outnames[ind]\n",
      "print inname, outname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Downloading...')\n",
      "t1 = time.clock()\n",
      "os.system('aws s3 cp '+s3name + ' .')\n",
      "td = time.clock() - t1\n",
      "print(td)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Loading into python...')\n",
      "t1 = time.clock()\n",
      "rawdata = nib.load(inname).get_data()\n",
      "tl = time.clock() - t1\n",
      "print(tl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Converting to flat file...')\n",
      "t1 = time.clock()\n",
      "dc.convscript(rawdata, tempname, (20, 20, 20))\n",
      "tc = time.clock() - t1\n",
      "print(tc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote to file...\n",
        "Copied to hadoop... temp/temp1_coil0.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Pickling...')\n",
      "t1 = time.clock()\n",
      "dc.VoxelPartition(textf = 'temp/'+tempname, cont = sc, parts = parts).save_as_pickle_file(outname)\n",
      "ts = time.clock() - t1\n",
      "print(ts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Transferring...')\n",
      "t1 = time.clock()\n",
      "os.system('./hadoop fs -getmerge ' + outname + ' ' + outname)\n",
      "os.system('aws s3 cp ' + outname + ' ' + s3outname)\n",
      "tt = time.clock() - t1\n",
      "print(tt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Cleaning up...')\n",
      "t1 = time.clock()\n",
      "os.system('rm ' + inname)\n",
      "os.system('rm ' + outname)\n",
      "os.system('./hadoop fs -rmr temp/' + tempname)\n",
      "os.system('./hadoop fs -rmr '+outname)\n",
      "tu = time.clock() - t1\n",
      "print(tu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning up...\n",
        "0.6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = ind + 1\n",
      "ind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Checking the result"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = npr.randint(0, 66)\n",
      "s3name = s3names[ind]\n",
      "inname = innames[ind]\n",
      "tempname = tempnames[ind]\n",
      "outname = outnames[ind]\n",
      "s3outname = s3outnames[ind]\n",
      "print inname, outname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8631_11_coil22_ec.nii.gz chris2_coil22.pickle\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.system('aws s3 cp ' + s3name + ' ' + inname)\n",
      "os.system('aws s3 cp ' + s3outname + ' ' + outname)\n",
      "os.system('./hadoop fs -put ' + outname + ' ' + outname)\n",
      "rawdata = nib.load(inname).get_data()\n",
      "tups = dc.VoxelPartition(picklef = outname, cont=sc).rdd.takeSample(False, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tup in tups:\n",
      "    dims = np.shape(tup[1])\n",
      "    x0 = npr.randint(0, dims[0])\n",
      "    x1 = npr.randint(0, dims[1])\n",
      "    x2 = npr.randint(0, dims[2])\n",
      "    coords = tup[0]\n",
      "    inds = npr.randint(0, 150, 10)\n",
      "    print(zip(rawdata[coords[0]+x0, coords[1]+x1, coords[2]+x2, inds],tup[1][x0, x1, x2, inds]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0.02546561, 0.025467), (0.058126066, 0.058136), (-0.058851123, -0.058838), (0.030249229, 0.030243), (-0.0033675851, -0.0033684), (0.23592588, 0.23596), (0.02546561, 0.025467), (-0.078685641, -0.078674), (0.028728668, 0.028732), (0.048238486, 0.048248)]\n",
        "[(0.0057273442, 0.0057259), (0.019396814, 0.019394), (-0.026825586, -0.026825), (0.0076509262, 0.0076523), (-0.0094105061, -0.009407), (-0.090302043, -0.090332), (-0.034675244, -0.034668), (-0.00034961468, -0.00034952), (-0.087137215, -0.087158), (0.077455625, 0.077454)]\n",
        "[(0.017299183, 0.017303), (-0.059632812, -0.059631), (0.031845622, 0.03186), (0.052054629, 0.052063), (-0.0028763618, -0.0028763), (-0.015064772, -0.015068), (-0.0028763618, -0.0028763), (0.060255516, 0.060242), (-0.0028763618, -0.0028763), (-0.0097794021, -0.0097809)]\n",
        "[(0.055676293, 0.055664), (0.069662675, 0.069641), (0.032435942, 0.03244), (-0.030223183, -0.030228), (-0.043910105, -0.043915), (-0.030223183, -0.030228), (0.037624981, 0.037628), (0.0029510113, 0.0029507), (0.0019154557, 0.0019159), (0.10706257, 0.10706)]\n",
        "[(0.037925813, 0.037933), (-0.085487433, -0.08551), (0.02989297, 0.029892), (0.035571054, 0.035583), (-0.055798203, -0.055786), (0.01355023, 0.01355), (0.10440326, 0.10443), (0.077675231, 0.077698), (-0.045903433, -0.045898), (0.073412754, 0.073425)]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The entire script (without ind)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import subprocess\n",
      "import donuts.spark.classes as dc\n",
      "import nibabel as nib\n",
      "import time\n",
      "\n",
      "os.chdir('/root/ephemeral-hdfs/bin')\n",
      "sz = (20, 20, 20)\n",
      "\n",
      "s3names = ['s3://rawpredator/chris1/8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['s3://rawpredator/chris2/8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "innames = ['8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "tempnames = ['temp1_coil' + str(i) + '.txt' for i in range(0, 33)] + \\\n",
      "            ['temp2_coil' + str(i) + '.txt' for i in range(0, 33)]\n",
      "outnames = ['chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "           ['chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]\n",
      "s3outnames = ['s3://chris1data/chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "             ['s3://chris2data/chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]\n",
      "    \n",
      "s3name = s3names[ind]\n",
      "inname = innames[ind]\n",
      "tempname = tempnames[ind]\n",
      "outname = outnames[ind]\n",
      "s3outname = s3outnames[ind]\n",
      "print inname, outname\n",
      "\n",
      "print('Downloading...')\n",
      "t1 = time.clock()\n",
      "os.system('aws s3 cp '+s3name + ' .')\n",
      "td = time.clock() - t1\n",
      "print(td)\n",
      "\n",
      "print('Loading into python...')\n",
      "t1 = time.clock()\n",
      "rawdata = nib.load(inname).get_data()\n",
      "tl = time.clock() - t1\n",
      "print(tl)\n",
      "\n",
      "print('Converting to flat file...')\n",
      "t1 = time.clock()\n",
      "dc.convscript(rawdata, tempname, (20, 20, 20))\n",
      "tc = time.clock() - t1\n",
      "print(tc)\n",
      "\n",
      "print('Pickling...')\n",
      "t1 = time.clock()\n",
      "dc.VoxelPartition(textf = 'temp/'+tempname, cont = sc, parts = parts).save_as_pickle_file(outname)\n",
      "ts = time.clock() - t1\n",
      "print(ts)\n",
      "\n",
      "print('Transferring...')\n",
      "t1 = time.clock()\n",
      "os.system('./hadoop fs -getmerge ' + outname + ' ' + outname)\n",
      "os.system('aws s3 cp ' + outname + ' ' + s3outname)\n",
      "tt = time.clock() - t1\n",
      "print(tt)\n",
      "\n",
      "print('Cleaning up...')\n",
      "t1 = time.clock()\n",
      "os.system('rm ' + inname)\n",
      "os.system('rm ' + outname)\n",
      "os.system('./hadoop fs -rmr temp/' + tempname)\n",
      "os.system('./hadoop fs -rmr '+outname)\n",
      "tu = time.clock() - t1\n",
      "print(tu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8631_5_coil1_ec.nii.gz chris1_coil1.pickle\n",
        "0.15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading...\n",
        "0.15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loading into python...\n",
        "8.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Converting to flat file...\n",
        "Wrote to file..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Copied to hadoop... temp/temp1_coil1.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "172.67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pickling...\n",
        "0.00999999999999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Transferring...\n",
        "0.29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up...\n",
        "0.57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Automate it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for ind in range(2, 66):\n",
      "    s3name = s3names[ind]\n",
      "    inname = innames[ind]\n",
      "    tempname = tempnames[ind]\n",
      "    outname = outnames[ind]\n",
      "    s3outname = s3outnames[ind]\n",
      "    print inname, outname\n",
      "\n",
      "    print('Downloading...')\n",
      "    t1 = time.clock()\n",
      "    os.system('aws s3 cp '+s3name + ' .')\n",
      "    td = time.clock() - t1\n",
      "    print(td)\n",
      "\n",
      "    print('Loading into python...')\n",
      "    t1 = time.clock()\n",
      "    rawdata = nib.load(inname).get_data()\n",
      "    tl = time.clock() - t1\n",
      "    print(tl)\n",
      "\n",
      "    print('Converting to flat file...')\n",
      "    t1 = time.clock()\n",
      "    dc.convscript(rawdata, tempname, (20, 20, 20))\n",
      "    tc = time.clock() - t1\n",
      "    print(tc)\n",
      "\n",
      "    print('Pickling...')\n",
      "    t1 = time.clock()\n",
      "    dc.VoxelPartition(textf = 'temp/'+tempname, cont = sc, parts = parts).save_as_pickle_file(outname)\n",
      "    ts = time.clock() - t1\n",
      "    print(ts)\n",
      "\n",
      "    print('Transferring...')\n",
      "    t1 = time.clock()\n",
      "    os.system('./hadoop fs -getmerge ' + outname + ' ' + outname)\n",
      "    os.system('aws s3 cp ' + outname + ' ' + s3outname)\n",
      "    tt = time.clock() - t1\n",
      "    print(tt)\n",
      "\n",
      "    print('Cleaning up...')\n",
      "    t1 = time.clock()\n",
      "    os.system('rm ' + inname)\n",
      "    os.system('rm ' + outname)\n",
      "    os.system('./hadoop fs -rmr temp/' + tempname)\n",
      "    os.system('./hadoop fs -rmr '+outname)\n",
      "    tu = time.clock() - t1\n",
      "    print(tu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}