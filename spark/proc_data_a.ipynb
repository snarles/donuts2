{
 "metadata": {
  "name": "",
  "signature": "sha256:7b8e0bc3b8b818385cb619af4bb1ca3898be4377ddc1d6723b5ab05da2f856dc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import subprocess\n",
      "import donuts.spark.classes as dc\n",
      "import nibabel as nib\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir('/root/ephemeral-hdfs/bin')\n",
      "sz = (20, 20, 20)\n",
      "parts = 30"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s3names = ['s3://rawpredator/chris1/8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['s3://rawpredator/chris2/8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "innames = ['8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "tempnames = ['temp1_coil' + str(i) + '.txt' for i in range(0, 33)] + \\\n",
      "            ['temp2_coil' + str(i) + '.txt' for i in range(0, 33)]\n",
      "outnames = ['chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "           ['chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]\n",
      "s3outnames = ['s3://chris1data/chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "             ['s3://chris2data/chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = 0\n",
      "s3name = s3names[ind]\n",
      "inname = innames[ind]\n",
      "tempname = tempnames[ind]\n",
      "outname = outnames[ind]\n",
      "s3outname = s3outnames[ind]\n",
      "print inname, outname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Downloading...')\n",
      "t1 = time.clock()\n",
      "os.system('aws s3 cp '+s3name + ' .')\n",
      "td = time.clock() - t1\n",
      "print(td)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Loading into python...')\n",
      "t1 = time.clock()\n",
      "rawdata = nib.load(inname).get_data()\n",
      "tl = time.clock() - t1\n",
      "print(tl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Converting to flat file...')\n",
      "t1 = time.clock()\n",
      "dc.convscript(rawdata, tempname, (20, 20, 20))\n",
      "tc = time.clock() - t1\n",
      "print(tc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote to file...\n",
        "Copied to hadoop... temp/temp1_coil0.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Pickling...')\n",
      "t1 = time.clock()\n",
      "dc.VoxelPartition(textf = 'temp/'+tempname, cont = sc, parts = parts).save_as_pickle_file(outname)\n",
      "ts = time.clock() - t1\n",
      "print(ts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Transferring...')\n",
      "t1 = time.clock()\n",
      "os.system('./hadoop fs -getmerge ' + outname + ' ' + outname)\n",
      "os.system('aws s3 cp ' + outname + ' ' + s3outname)\n",
      "tt = time.clock() - t1\n",
      "print(tt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Cleaning up...')\n",
      "t1 = time.clock()\n",
      "os.system('rm ' + inname)\n",
      "os.system('rm ' + outname)\n",
      "os.system('./hadoop fs -rmr temp/' + tempname)\n",
      "os.system('./hadoop fs -rmr '+outname)\n",
      "tu = time.clock() - t1\n",
      "print(tu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning up...\n",
        "0.6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = ind + 1\n",
      "ind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Checking the result"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.system('aws s3 cp ' + s3name + ' ' + inname)\n",
      "os.system('aws s3 cp ' + s3outname + ' ' + outname)\n",
      "os.system('./hadoop fs -put ' + outname + ' ' + outname)\n",
      "rawdata = nib.load(inname).get_data()\n",
      "tup = dc.VoxelPartition(picklef = outname, cont=sc).rdd.takeSample(False, 1)[0]\n",
      "coords = tup[0]\n",
      "print(rawdata[coords[0], coords[1], coords[2], 0:150:10])\n",
      "print(tup[1][0, 0, 0, 0:150:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The entire script (without ind)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import subprocess\n",
      "import donuts.spark.classes as dc\n",
      "import nibabel as nib\n",
      "import time\n",
      "\n",
      "os.chdir('/root/ephemeral-hdfs/bin')\n",
      "sz = (20, 20, 20)\n",
      "\n",
      "s3names = ['s3://rawpredator/chris1/8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['s3://rawpredator/chris2/8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "innames = ['8631_5_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)] + \\\n",
      "          ['8631_11_coil' + str(i) + '_ec.nii.gz' for i in range(0, 33)]\n",
      "tempnames = ['temp1_coil' + str(i) + '.txt' for i in range(0, 33)] + \\\n",
      "            ['temp2_coil' + str(i) + '.txt' for i in range(0, 33)]\n",
      "outnames = ['chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "           ['chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]\n",
      "s3outnames = ['s3://chris1data/chris1_coil' + str(i) + '.pickle' for i in range(0, 33)] + \\\n",
      "             ['s3://chris2data/chris2_coil' + str(i) + '.pickle' for i in range(0, 33)]\n",
      "    \n",
      "s3name = s3names[ind]\n",
      "inname = innames[ind]\n",
      "tempname = tempnames[ind]\n",
      "outname = outnames[ind]\n",
      "s3outname = s3outnames[ind]\n",
      "print inname, outname\n",
      "\n",
      "print('Downloading...')\n",
      "t1 = time.clock()\n",
      "os.system('aws s3 cp '+s3name + ' .')\n",
      "td = time.clock() - t1\n",
      "print(td)\n",
      "\n",
      "print('Loading into python...')\n",
      "t1 = time.clock()\n",
      "rawdata = nib.load(inname).get_data()\n",
      "tl = time.clock() - t1\n",
      "print(tl)\n",
      "\n",
      "print('Converting to flat file...')\n",
      "t1 = time.clock()\n",
      "dc.convscript(rawdata, tempname, (20, 20, 20))\n",
      "tc = time.clock() - t1\n",
      "print(tc)\n",
      "\n",
      "print('Pickling...')\n",
      "t1 = time.clock()\n",
      "dc.VoxelPartition(textf = 'temp/'+tempname, cont = sc, parts = parts).save_as_pickle_file(outname)\n",
      "ts = time.clock() - t1\n",
      "print(ts)\n",
      "\n",
      "print('Transferring...')\n",
      "t1 = time.clock()\n",
      "os.system('./hadoop fs -getmerge ' + outname + ' ' + outname)\n",
      "os.system('aws s3 cp ' + outname + ' ' + s3outname)\n",
      "tt = time.clock() - t1\n",
      "print(tt)\n",
      "\n",
      "print('Cleaning up...')\n",
      "t1 = time.clock()\n",
      "os.system('rm ' + inname)\n",
      "os.system('rm ' + outname)\n",
      "os.system('./hadoop fs -rmr temp/' + tempname)\n",
      "os.system('./hadoop fs -rmr '+outname)\n",
      "tu = time.clock() - t1\n",
      "print(tu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8631_5_coil1_ec.nii.gz chris1_coil1.pickle\n",
        "0.15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloading...\n",
        "0.15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loading into python...\n",
        "8.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Converting to flat file...\n",
        "Wrote to file..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Copied to hadoop... temp/temp1_coil1.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "172.67"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Pickling...\n",
        "0.00999999999999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Transferring...\n",
        "0.29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cleaning up...\n",
        "0.57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Automate it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for ind in range(2, 66):\n",
      "    s3name = s3names[ind]\n",
      "    inname = innames[ind]\n",
      "    tempname = tempnames[ind]\n",
      "    outname = outnames[ind]\n",
      "    s3outname = s3outnames[ind]\n",
      "    print inname, outname\n",
      "\n",
      "    print('Downloading...')\n",
      "    t1 = time.clock()\n",
      "    os.system('aws s3 cp '+s3name + ' .')\n",
      "    td = time.clock() - t1\n",
      "    print(td)\n",
      "\n",
      "    print('Loading into python...')\n",
      "    t1 = time.clock()\n",
      "    rawdata = nib.load(inname).get_data()\n",
      "    tl = time.clock() - t1\n",
      "    print(tl)\n",
      "\n",
      "    print('Converting to flat file...')\n",
      "    t1 = time.clock()\n",
      "    dc.convscript(rawdata, tempname, (20, 20, 20))\n",
      "    tc = time.clock() - t1\n",
      "    print(tc)\n",
      "\n",
      "    print('Pickling...')\n",
      "    t1 = time.clock()\n",
      "    dc.VoxelPartition(textf = 'temp/'+tempname, cont = sc, parts = parts).save_as_pickle_file(outname)\n",
      "    ts = time.clock() - t1\n",
      "    print(ts)\n",
      "\n",
      "    print('Transferring...')\n",
      "    t1 = time.clock()\n",
      "    os.system('./hadoop fs -getmerge ' + outname + ' ' + outname)\n",
      "    os.system('aws s3 cp ' + outname + ' ' + s3outname)\n",
      "    tt = time.clock() - t1\n",
      "    print(tt)\n",
      "\n",
      "    print('Cleaning up...')\n",
      "    t1 = time.clock()\n",
      "    os.system('rm ' + inname)\n",
      "    os.system('rm ' + outname)\n",
      "    os.system('./hadoop fs -rmr temp/' + tempname)\n",
      "    os.system('./hadoop fs -rmr '+outname)\n",
      "    tu = time.clock() - t1\n",
      "    print(tu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "8\n",
        "9\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}