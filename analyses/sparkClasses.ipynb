{
 "metadata": {
  "name": "",
  "signature": "sha256:e435a048288d716bae296773f172c6d953784b14f50334a75f82c4591a651824"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Development of classes for Spark data analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import donuts.spark.classes as ds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load some numpy data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def readN(filename, n):\n",
      "    with open(filename, 'r') as myfile:\n",
      "        head = [next(myfile).strip() for x in xrange(n)]\n",
      "    return head\n",
      "\n",
      "def csvrow2array(st):\n",
      "    return np.array([float(s) for s in st.replace(',',' ').replace('  ',' ').split(' ')])\n",
      "\n",
      "def int2str(z):\n",
      "    if (z < 90):\n",
      "        return chr(z+33)\n",
      "    else:\n",
      "        resid = int(z % 90)\n",
      "        z = int(z-resid)/90\n",
      "        return int2str(z)+chr(90+33)+chr(resid+33)\n",
      "    \n",
      "def ints2str(zs):\n",
      "    return ''.join(int2str(z) for z in zs)\n",
      "\n",
      "def str2ints(st):\n",
      "    os = [ord(c)-33 for c in st]\n",
      "    zs = []\n",
      "    counter = 0\n",
      "    while counter < len(os):\n",
      "        if os[counter] == 90:\n",
      "            zs[-1] = zs[-1] * 90 + os[counter + 1]\n",
      "            counter = counter + 1\n",
      "        else:\n",
      "            zs.append(os[counter])\n",
      "        counter = counter + 1\n",
      "    return zs\n",
      "\n",
      "def str2array(st):\n",
      "    pts = st.split('|')\n",
      "    arr = np.array([str2ints(pt) for pt in pts]).T\n",
      "    return arr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csv1 = readN('/root/data/old_data.csv', 1000)\n",
      "cff1 = readN('/root/data/chris2_comb.cff', 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x):\n",
      "    lala = ds.Voxel(x)\n",
      "    lala.setConversion(minVal = 0.0)\n",
      "    return lala"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "voxes = sc.parallelize(cff1[:10],2).map(f).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "voxes[5].getCoords()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "(0, 0, 5)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v2=voxes[5].convertData(np.array([5.5, 1.1]), intRes = 10.0, minVal = -1.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "('!!&', '!!&b6')"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v2.getData()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "array([ 5.5,  1.1])"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type('haha')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "str"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds.Voxel(cff1[10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "('!', '!')"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(['a','b'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "['a', 'b']"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[c for c in list(['aa'])[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "['a', 'a']"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Surrogate for SparkContext /RDD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FakeRDD:\n",
      "    partitions = []\n",
      "    def __init__(self, partitions):\n",
      "        self.partitions = partitions\n",
      "        \n",
      "    def map(self, func):\n",
      "        newpartitions = []\n",
      "        for partition in self.partitions:\n",
      "            newpartition = [func(element) for element in partition]\n",
      "            newpartitions.append(newpartition)\n",
      "        return FakeRDD(newpartitions)\n",
      "    \n",
      "    def flatMap(self, func):\n",
      "        newpartitions = []\n",
      "        for partition in self.partitions:\n",
      "            newpartition = []\n",
      "            for element in partition:\n",
      "                newpartition = newpartition + func(element)\n",
      "            newpartitions.append(newpartition)\n",
      "        return FakeRDD(newpartitions)        \n",
      "    \n",
      "    def mapPartitions(self, func):\n",
      "        newpartitions = []\n",
      "        for partition in self.partitions:\n",
      "            newpartitions.append(func(iter(partition)))\n",
      "        return FakeRDD(newpartitions)   \n",
      "    \n",
      "    def reduce(self, func):\n",
      "        a_list = self.collect()\n",
      "        n_items = len(a_list)\n",
      "        if n_items < 2:\n",
      "            return []\n",
      "        ans = func(a_list[0], a_list[1])\n",
      "        for ind in range(2, n_items):\n",
      "            ans = func(ans, a_list[ind])\n",
      "        return ans\n",
      "    \n",
      "    def collect(self):\n",
      "        ans = []\n",
      "        for partition in self.partitions:\n",
      "            ans = ans + partition\n",
      "        return ans\n",
      "    \n",
      "    def cache(self):\n",
      "        return\n",
      "    \n",
      "class FakeSparkContext:\n",
      "    name = ''\n",
      "    def __init__(self, name = 'FakeContext'):\n",
      "        self.name = name\n",
      "        \n",
      "    def textFile(self, tf, n_parts):\n",
      "        ff = open(tf, 'r')\n",
      "        a_list = ff.read().split('\\n')\n",
      "        ff.close()\n",
      "        return self.parallelize(a_list, n_parts)\n",
      "    \n",
      "    def parallelize(self, a_list, n_parts):\n",
      "        n_items = len(a_list)\n",
      "        sz_part = n_items/n_parts + 1\n",
      "        count = 0\n",
      "        partitions = []\n",
      "        for ind in range(n_parts):\n",
      "            newcount = min(count + sz_part, n_items)\n",
      "            newpartition = a_list[count:newcount]\n",
      "            partitions.append(newpartition)\n",
      "            count = newcount\n",
      "        return FakeRDD(partitions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_list = [1,5,4,6,8,8]\n",
      "sc = FakeSparkContext()\n",
      "rdd = sc.parallellize(a_list, 3)\n",
      "from operator import add\n",
      "rdd.reduce(add)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "32"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 99,
       "text": [
        "[1, 5, 4, 6, 8, 8]"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iter([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "<listiterator at 0x7f193ef48050>"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc0 = FakeSparkContext()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Test Spark stuff"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}