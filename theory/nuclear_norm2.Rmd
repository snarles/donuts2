---
title: "NNLS with Nuclear Norm"
author: "Charles Zheng"
date: "06/13/2015"
output: html_document
---

Consider the problem

$$
Y = XB + E
$$
where $Y_{n \times v}$, $X_{n \times p}$ are known, while $B_{p \times v} \geq 0$ and $E_{n \times v}$ low-rank are unknown.

Our goal is to recover the unknown $B$ from observed $Y$ and $X$.
We solve the convex relaxation
$$
\text{minimize } ||Y - XB - E||_F^2 + \lambda ||E||_* \text{ subject to }B\geq 0
$$
where $||\cdot||_*$ is the nuclear norm.

Alternatively, define $P_C$ to be the projection onto the cone $C = \{Z: Z = XU\text{ for }U \geq 0\}$.
Then one way of solving the optimization problem is to define $Z = Y-E$ and solve
$$
\text{minimize } ||Z - P_C Z||^2_F + \lambda ||Y - Z||_*
$$
then finding $B$ which minimizes $||Z - XB||^2_F$ subject to $B \geq 0$.

## Boosting algorithm

We solve the problem by building a path of solutions with nuclear norm generally increasing from 0,
and stopping when we have found the optimal nuclear norm.
This type of algorithm can be seen in the chapter on boosting in Elements of Statistical Learning.
Choosing $\epsilon$. At the $k$th iteration, let
$$
Z^{(k)} = \sum_{i=1}^k \epsilon \Delta_k
$$
where $\Delta_k$ is a rank-1 matrix with $||\Delta_k|| = 1$.
Then the nuclear norm is bounded as $||Z^{(k)}||_* \leq k\epsilon$ but may be less.
The $\Delta_k$ is chosen at each step as either:

 * The negative of one of the previous $\Delta_1, ..., \Delta_{k-1}$
 * A new direction, based on the rank-1 approximation of the negative unconstrained gradient.
 
The negative unconstrained gradient in this problem is $P_C Z - Z$ (see `nuclear_norm.Rmd`).
The new direction $\Delta_k$ is thus formed by taking the singular-value decomposition $P_C Z - Z = UDV^T$ and taking $\Delta_k = U_1 V_1^T$.
The choice of whether to take the negative of a previous direction or a new direction is determined by minimizing the projection distance $||Z - P_C Z||$.

The actual implementation is slightly different in the sense that $Z$ is maintained as a sum of the "active set" of directions
$$Z^{(k)} = \sum_{i=1}^k \epsilon w_i \Delta_i.$$
When $\Delta_k$ is added to the active set, we find the weight $w_k$ which approximately minimizes $||P_C (Z + w_k \Delta_k) - (Z + w_k \Delta_k)||^2$, but we update $Z$ by $\epsilon w_k \Delta_k$ rather than the full step $w_k \Delta_k$.  Rather than adding the negative of a previous direction from the "active set", we reduce its the weight $w_k$: if the weight is reduced to below zero, then we remove the direction $\Delta_k$.

At each iteration $k$ we track the objective function, and the minimum so far.  Stop if we exceed the running minimum by some threshold.  (Note the objective may be non-monotonic due to discretization error).

## Constrained approach

`nuclear_norm.Rmd` describes an algorithm for solving the constrained-form problem

$$
\text{minimize } ||V - P_C V||^2_F \text{ subject to }||Y - V||_* \leq c
$$

as well as biconvex approaches for the (nonconvex) low-rank constraint.

### Requirements

```{r}
library(pracma)
library(magrittr)
library(nnls)
library(parallel)
library(bayesm)
```

### Random stuff
Nuclear norm of a square gaussian random matrix is nearly linear with a slope of 4.
A $p \times n$ rectangular gaussian matrix with $p > n$ has about $\sqrt{p/n}$ times the nuclear norm of an $n \times n$ matrix.

```{r}
nnorm <- function(E) sum(svd(E, nu = 0, nv = 0)$d)
fnorm2 <- function(E) sum(E^2)

ann <- numeric(20)
for (k in 1:20) {
  ann[k] <- mclapply(1:1000, function(i) nnorm(randn(k, k)), mc.cores = 3) %>% unlist %>% mean
}
plot(ann, type = "l", main = "Nuclear norm of random matrix")
tt <- 1:20
lm(ann ~ tt)
```

## Example Problem


Let X be randomly generated, B sparse, E rank 2.

```{r}
n <- 30
p <- 40
v <- 30
sparsity <- 0.1
sigmaE <- 0.1
sigmaI <- 0.05 # isotropic noise
rk <- 1
X <- randn(n, p) %>% abs
B <- randn(p, v) %>% abs * (rand(p, v) < sparsity)
U <- sqrt(sigmaE/rk) * randn(n, rk)
V <- sqrt(sigmaE/rk) * randn(rk, v)
E <- U %*% V
mu <- X %*% B
Y <- mu + E + sigmaI * randn(n, v)
```

Compare the squared Frobenius norm of a $n \times v$ gaussian matrix with its nuclear norm.

```{r}
res <- mclapply(1:1000, function(i) {
  xx <- randn(n, v)
  c(sum(xx^2), nnorm(xx))
}, mc.cores = 3)
res <- do.call(rbind, res)
colMeans(res)
(f2n <- colMeans(res) %>% {.[1]/.[2]})
```

The $\lambda$ for nuclear norm penalty should be at least this value to ensure comparability between NNLS and nuclear norm optimization.

Compare the Frob norm of the error with the nuclear norm.
```{r}
nnorm(E)
sum(E^2)
nnorm(E) * f2n
```

CHeck that the 


### Use NNLS to recover B
```{r}
multi_nnls <- function(X0, Y0, l1p = 0, mc.cores = 3) {
  v <- dim(Y)[2]
  Y <- rbind(Y, 0)
  X <- rbind(X0, l1p * rep(1, p))
  bs <- mclapply(1:v, function(i) nnls(X, Y[, i])$x, mc.cores = mc.cores)
  do.call(cbind, bs)
}
l1p <- 0.1
B_nnls <- multi_nnls(X, Y, l1p)
#Check denoising error
mu_nnls <- X %*% B_nnls
(err_nnls <- sum((mu_nnls - mu)^2))
```


### Boosting


```{r}
nuclear_boost <- function(X, Y, eps, l1p = 0, maxits = 30, mc.cores = 3) {
  v <- dim(Y)[2]
  p <- dim(X)[2]
  n <- dim(X)[1]
  Z <- Y
  objective <- function(B, Z) {
    sum((rbind(Z, 0) - rbind(X, l1p) %*% B)^2)
  }
  objs <- numeric(maxits)
  nnorms <- numeric(maxits)
  us <- matrix(0, n, maxits)
  vs <- matrix(0, v, maxits)
  for (it in 1:maxits) {
    # find the projection
    B <- multi_nnls(rbind(X, l1p), rbind(Z, 0), mc.cores = mc.cores)
    Zh <- X %*% B
    resid <- Z - Zh
    res <- svd(-resid, nu = 1, nv = 1)
    us[, it] <- res$u
    vs[, it] <- res$v
    Z <- Z  + eps * us[, it] %*% t(vs[, it])
    # objective
    objs[it] <- objective(B, Z)
    nnorms[it] <- nnorm(Z - Y)
  }
  E <- Z - Y
  solution_path <- function(i) {
    Z <- Y
    for (k in 1:i) {
      Z <- Z + eps * us[, k] %*% t(vs[, k])
    }
    B <- multi_nnls(X, Z, mc.cores = mc.cores)
    list(B = B, E = (Z - Y)[1:n, ])
  }
  list(E = E, B = B, objs = objs, nnorms = nnorms, 
       objective = objective, us = us, vs = vs, eps = eps,
       solution_path = solution_path)
}
```


The step size has to be carefully tuned to avoid disaster.

```{r}
lambda <- 7
res_nn <- nuclear_boost(X, Y, eps = 0.03, l1p = l1p, maxits = 300, mc.cores = 3)
plot(res_nn$objs, type = "l", main = "Objective")
plot(res_nn$nnorms, type = "l", main = "Nuclear norm")
plot(res_nn$objs + lambda * res_nn$nnorms, type = "l", main = "Penalized objective")
```

The number of unique directions is small.
```{r}
matplot(t(res_nn$us), pch = "-", main = "Directions")
```

The solution at some point on the path
```{r}
ind <- order(res_nn$objs + lambda * res_nn$nnorms)[1]
(n_star <- res_nn$nnorms[ind])
res_ind <- res_nn$solution_path(ind)
nnorm(res_ind$E)
B_nn <- res_ind$B
mu_nn <- X %*% res_nn$B
(err_nn <- sum((mu_nn - mu))^2)
```

### Constrained form

```{r}
rank1approx <- function(E) {
  res <- svd(E, nu = 1, nv = 1)
  res$u %*% t(res$v)
}

nuclear_opt <- function(X, Y, l1p, lambda, maxits = 10, mc.cores = 3) {
  v <- dim(Y)[2]
  p <- dim(X)[2]
  n <- dim(X)[1]
  Z <- Y
  objective <- function(B, Z) {
    sum((rbind(Z, 0) - rbind(X, l1p) %*% B)^2)
  }
  objs <- c()
  nnorms <- c()
  for (it in 1:maxits) {
    # find the projection
    B <- multi_nnls(rbind(X, l1p), rbind(Z, 0), mc.cores = mc.cores)
    Zh <- X %*% B
    resid <- Z - Zh
    alpha <- 1/it
    Z <- Y + (1-alpha) * (Z - Y)  + alpha * lambda * rank1approx(-resid)
    # objective
    objs <- c(objs, objective(B, Z))
    nnorms <- c(nnorms, nnorm(Z - Y))
  }
  list(E = Z - Y, B = B, objs = objs, nnorms = nnorms, objective = objective)
}
```
check that we got the solution
```{r}
res_nnc <- nuclear_opt(X, Y, l1p, n_star, 100)
plot(res_nnc$objs, type = "l", main = "Objective")
plot(res_nnc$nnorms, type = "l", main = "Nuclear norm")
c(res_nnc$objective(B, mu), nnorm(mu - Y))
mu_nnc <- X %*% res_nnc$B
(err_nnc <- sum((mu_nnc - mu))^2)
plot(res_nnc$B, res_ind$B, main = "Boosting vs. exact")
```

### Comparison

```{r}
err_nnls
err_nn
```